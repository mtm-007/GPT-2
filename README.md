# Transformers
## GPT- 2 (124M parameter) 

# Full Transformers Architecture 
## This repo 

- took inspiration and reference from the Great Andrej Karphaty,it is not a direct clone but written as a follow up with his videos and code repo on my own understanding with some modefication
- Andrej Karpathy is the best on educating complex systems by on scaling it down and start from the basic on first principles approach, Long live our Karpathy

# not a clone
- The reason for not directly forking or cloning andrej Karpathy repo is to learn and experince the pain point when scaling and pushing the limits of code(i.e like `torch.cuda.OutOfMemoryError: CUDA out of memory` errors) those pain points are part of the learning curve, other developers might hate and avoid those but those errors hold the power

- This repo also covers the Full Transformers architecture as the paper with encoder and decoder blocks for language translation, which isnt covered by The GREAT Karpathy videos, now in a another PR branch will later be merged

# Experiment Tracking
- for experment tracking the repo uses `Wandb` on the alternative  `mlflow` can be used to its fully opensource and open governance   

